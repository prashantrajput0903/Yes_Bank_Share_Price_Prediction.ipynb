{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "PH-0ReGfmX4f",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "JcMwzZxoAimU",
        "8G2x9gOozGDZ",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prashantrajput0903/Yes_Bank_Share_Price_Prediction.ipynb/blob/main/Sample_EDA_Submission_Template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - \n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - supervised\n",
        "##### **Contribution**    - Individual\n",
        "##### **Team Member  -**    Prashant Kumar\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/prashantrajput0903/Yes_Bank_Share_Price_Prediction.ipynb\n",
        "\n"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes Bank is a well-known bank in the Indian financial domain. Since 2018, it has been in the news because of the fraud case involving Rana Kapoor. Owing to this fact, it was interesting to see how that impacted the stock prices of the company and whether Time series models or any other predictive models can do justice to such situations. This dataset has monthly stock prices of the bank since its inception and includes closing, starting, highest, and lowest stock prices of every month.\n",
        "Our main objective is to predict the stockâ€™s closing price of the month."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Loading the libraries and the data."
      ],
      "metadata": {
        "id": "wmn4q55ahjRI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing the libraries we'll need.\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "7eYnAcb3hgLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dataset Loading"
      ],
      "metadata": {
        "id": "prl8IJ8yh3ND"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "     "
      ],
      "metadata": {
        "id": "mQGcS04shmxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/data_YesBank_StockPrices (1).csv')\n"
      ],
      "metadata": {
        "id": "KuqQ-ufNh2AJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Taking a look at the data.\n",
        "df.head()          # displays first five instances of the dataframe."
      ],
      "metadata": {
        "id": "S1DkxJfGiqtf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explaining the data:-\n",
        "We have a dataset containing values of Yes bank monthly stock prices as mentioned in our problem statement. \n",
        "\n",
        "Explaining the features present :-\n",
        "\n",
        "\n",
        "*  **Date :-** The date (Month and Year provided)\n",
        "*  **Open :-** The price of the stock at the beginning of a particular time period.\n",
        "*  **High :-**The Peak(Maximum) price at which a stock traded during the period.\n",
        "*  **Low :-**The Lowest price at which a stock traded during the period.\n",
        "*  **Close :-** The trading price at the end (in this case end of the month)."
      ],
      "metadata": {
        "id": "dRRR_E0TjCar"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Cleaning.**"
      ],
      "metadata": {
        "id": "q9sRkmr6jHWh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking for null values.\n",
        "df.isna().sum()"
      ],
      "metadata": {
        "id": "xLXWb3nai-P-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# So there are no null values in our dataset.\n",
        "# Getting information about our data - its datatypes, its size etc. also printing the shape of the data.\n",
        "df.info()\n",
        "print('\\n', f'The shape of the dataset is : {df.shape}')"
      ],
      "metadata": {
        "id": "9koR5pFCjKce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# getting descriptive statistics of the data.\n",
        "df.describe(include='all')"
      ],
      "metadata": {
        "id": "S-Vakht3jMM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# as we can see, Date column has the object datatype. \n",
        "df['Date']"
      ],
      "metadata": {
        "id": "NVcmy8FAjOaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we need to modify this before passing it to a model.\n",
        "# lets convert Date column to a proper datetime datatype.\n",
        "from datetime import datetime\n",
        "df['Date'] = pd.to_datetime(df['Date'].apply(lambda x: datetime.strptime(x, '%b-%y')))     # this converts date to a yyyy-mm-dd format."
      ],
      "metadata": {
        "id": "atxfFjAojQLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "58J-9d1gjTds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we are trying to track variation in stock price on different dates, it makes sense to set this column as index."
      ],
      "metadata": {
        "id": "iqtTaftbjXqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.set_index('Date', inplace=True)           # setting Date column as index."
      ],
      "metadata": {
        "id": "1G4Yc6OyjVDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking the data.\n",
        "df.head()"
      ],
      "metadata": {
        "id": "09UwanWqjbBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see from the dataframe above, all the columns we have contain numerical \n",
        "\n",
        "\n",
        "data. There is no categorical data present.\n"
      ],
      "metadata": {
        "id": "Od-JDdxFjh4j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking all features for presence of outliers.\n",
        "for col in df.columns:\n",
        "  plt.figure(figsize=(7,5))\n",
        "  sns.boxplot(df[col])\n",
        "  plt.xlabel(col, fontsize=13)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "tE9cwGebjeHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see there are some outliers present in our data. We will need to deal with these before proceeding to modelling."
      ],
      "metadata": {
        "id": "sj3GNYt2jsIw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#EDA\n"
      ],
      "metadata": {
        "id": "DzjaYmwrjuAr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Separating the dependent and independent variables.\n",
        "independent_variables = df.columns.tolist()[:-1]\n",
        "dependent_variable = ['Close']\n",
        "\n",
        "print(independent_variables)\n",
        "print(dependent_variable)"
      ],
      "metadata": {
        "id": "IQfYUqXZjnjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the dependent variable .\n",
        "plt.figure(figsize=(12,7))\n",
        "df['Close'].plot(color = 'r')\n",
        "plt.grid(which='major', linestyle='-', linewidth='0.5', color='green')\n",
        "plt.grid(which='minor', linestyle=':', linewidth='0.5', color='green')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Closing Price')\n",
        "plt.title('Closing Price with Date')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8LJ5Ya30jxcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the stock price is rising up until 2018 when the fraud case involving Rana Kapoor happened after which the stock price has had a sharp decline."
      ],
      "metadata": {
        "id": "bOPg9up-j3BO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the distributions of all features.\n",
        "for col in df.columns:\n",
        "  plt.figure(figsize=(10,6))\n",
        "  sns.distplot(df[col], color='y')\n",
        "  plt.xlabel(col, fontsize=13)\n",
        "  plt.ylabel('count')\n",
        "\n",
        "  # Plotting the mean and the median.\n",
        "  plt.axvline(df[col].mean(),color='green',linewidth=2)                            # axvline plots a vertical line at a value (mean in this case). \n",
        "  plt.axvline(df[col].median(),color='red',linestyle='dashed',linewidth=1.5)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "aXm-Vjgfjztp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can clearly see that ***these distributions are positively skewed***. The mean and median are at significant distance from each other.\n",
        "\n",
        " So we need to transform them into something close to a Normal Distribution as our models give optimal results that way."
      ],
      "metadata": {
        "id": "clvHTNf8j9N_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets use log transformation on these features using np.log() and plot them.\n",
        "for col in df.columns:\n",
        "  plt.figure(figsize=(10,6))\n",
        "  sns.distplot(np.log10(df[col]), color='y')\n",
        "  plt.xlabel(col, fontsize=13)\n",
        "  plt.ylabel('count')\n",
        "\n",
        "  # Plotting the mean and the median.\n",
        "  plt.axvline(np.log10(df[col]).mean(),color='green',linewidth=2) \n",
        "  plt.axvline(np.log10(df[col]).median(),color='red',linestyle='dashed',linewidth=1.5)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "woTvnOouj5mT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, the distributions are very similar to Normal distribution. The mean and median values are nearly same.\n",
        "\n"
      ],
      "metadata": {
        "id": "4_jfIP-qkGu5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's check for outliers now in the transformed variable data.\n",
        "for col in df.columns:\n",
        "  plt.figure(figsize=(7,7))\n",
        "  sns.boxplot(np.log10(df[col]))\n",
        "  plt.xlabel(col, fontsize=13)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "GX5BQRdekBG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ow, we have no outliers anymore. Log transformation diminishes the outlier's effect. \n",
        "\n",
        "***Since we have a very small dataset to work with, dropping the outliers completely is not a good idea.*** So this is how we are going to leave them."
      ],
      "metadata": {
        "id": "mToAVQXakOB8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the independent variables against dependent variable close and also checking the correlation between them.\n",
        "for col in independent_variables:\n",
        "\n",
        "  fig = plt.figure(figsize=(12, 6))\n",
        "  ax = fig.gca()\n",
        "  feature = df[col]\n",
        "  label = df['Close']\n",
        "  correlation = feature.corr(label)        # calculating the correlation between dependent variable and independent features.\n",
        "  plt.scatter(x=feature, y=label)          # plotting dependent variables against independent features.  \n",
        "\n",
        "  # Setting the x,y labels and the title.\n",
        "  plt.xlabel(col)\n",
        "  plt.ylabel('Close')\n",
        "  ax.set_title('Close vs ' + col + '- correlation: ' + str(round((correlation),4)))\n",
        "\n",
        "  z = np.polyfit(df[col], df['Close'], 1)                                \n",
        "  y_hat = np.poly1d(z)(df[col])\n",
        "\n",
        "  plt.plot(df[col], y_hat, \"r--\", lw=1)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uZ0GhbGbkKLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that all of our independent variables are highly correlated to the dependent variable.\n",
        "\n",
        "And the relationship between dependent and independent variables is linear in nature."
      ],
      "metadata": {
        "id": "eZ8U3JL5kYFq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now let's visualise for the correlation among all variables.\n",
        "corr = df.corr()\n",
        "plt.figure(figsize=(16,7))\n",
        "sns.heatmap(corr, annot=True, cmap='coolwarm')"
      ],
      "metadata": {
        "id": "7E39bbaZkUZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the heatmap above, we can clearly see that there is a very high correlation between each pair of features in our dataset. While it is desirable for the dependent variable to be highly correlated with independent variables, the independent varibles should ideally not have high correlation with one another.\n",
        "\n",
        "***This causes a problem for us as high correlation among independent variables (multicollinearity) is a problem for our models.***"
      ],
      "metadata": {
        "id": "coqcN8bFkfGd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's visualise the relationship between each pair of variables using pair plots.\n",
        "sns.pairplot(df)"
      ],
      "metadata": {
        "id": "pslf0HujkbDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Preprocessing**\n"
      ],
      "metadata": {
        "id": "mEZsd545kmMy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dealing with multicollinearity using VIF analysis.\n",
        "# Calculating VIF(Variation Inflation Factor) to see the correlation between independent variables\n",
        "\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor  \n",
        "\n",
        "def calc_vif(X):\n",
        "\n",
        "    # Calculating VIF\n",
        "    vif = pd.DataFrame()\n",
        "    vif[\"variables\"] = X.columns\n",
        "    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        "\n",
        "    return(vif)"
      ],
      "metadata": {
        "id": "2Cp1FZqKkjhh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calc_vif(df[[i for i in df.describe().columns if i not in ['Date','Close']]])"
      ],
      "metadata": {
        "id": "4oStgzsXksJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see the values of VIF factor are very high. However since the dataset is so small and has just 3 independent features, multicollinearity is unavoidable here as any feature engineering will lead to loss of information."
      ],
      "metadata": {
        "id": "aEMKh7eykyS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating arrays of our input variable and label to feed the data to the model.\n",
        "# Create the data of independent variables\n",
        "x = np.log10(df[independent_variables]).values            # applying log transform on our independent variables.\n",
        "\n",
        "# Create the dependent variable data\n",
        "y = np.log10(df[dependent_variable]).values               # applying log transform on our dependent variable."
      ],
      "metadata": {
        "id": "Rx4nBwVBkuDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# splitting the data into a train and a test set. we do this using train test split.\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)       # we keep 20% of the data in test set."
      ],
      "metadata": {
        "id": "hH-qzXWvkwI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "***Scaling the data is very important for us so as to avoid giving more importance to features with large values. This is achieved by normalization or standardization of the data.***"
      ],
      "metadata": {
        "id": "L3h1xw79k6Zr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling the data.\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "x_train = scaler.fit_transform(x_train)\n",
        "x_test = scaler.transform(x_test)"
      ],
      "metadata": {
        "id": "mwFXp7R6k3hk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking the values.\n",
        "x_train[0:10]"
      ],
      "metadata": {
        "id": "AbNqBbM2k_Mg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Linear Regression**\n"
      ],
      "metadata": {
        "id": "vGgT1IbllDKH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing LinearRegression model and the metrics that we will use for evaluating different models performance.\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error"
      ],
      "metadata": {
        "id": "7rT9LMRtlA3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing the model.\n",
        "model_lr = LinearRegression()\n",
        "\n",
        "# Fitting the model on our train data.\n",
        "model_lr.fit(x_train, y_train)"
      ],
      "metadata": {
        "id": "jWkug8mOlJR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicting on our test data.\n",
        "y_pred_linear = model_lr.predict(x_test)"
      ],
      "metadata": {
        "id": "X4AF_2_DlLAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the model parameters. printing the intercept.\n",
        "model_lr.intercept_"
      ],
      "metadata": {
        "id": "1uT29PW5lMqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# printing the model coefficients.\n",
        "model_lr.coef_"
      ],
      "metadata": {
        "id": "wDUZLLzllO5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating the performance metrics.\n",
        "MAE_linear = round(mean_absolute_error(10**(y_test),(10**y_pred_linear)),4)\n",
        "print(f\"Mean Absolute Error : {MAE_linear}\")\n",
        "\n",
        "MSE_linear = round(mean_squared_error((10**y_test),10**(y_pred_linear)),4)\n",
        "print(f\"Mean squared Error : {MSE_linear}\")\n",
        "\n",
        "RMSE_linear = round(np.sqrt(MSE_linear),4)\n",
        "print(f\"Root Mean squared Error : {RMSE_linear}\")\n",
        "\n",
        "R2_linear = round(r2_score(10**(y_test), 10**(y_pred_linear)),4)\n",
        "print(f\"R2 score : {R2_linear}\")\n",
        "\n",
        "Adjusted_R2_linear = round(1-(1-r2_score(10**y_test,10**y_pred_linear))*((x_test.shape[0]-1)/(x_test.shape[0]-x_test.shape[1]-1)),3)\n",
        "print(f\"Adjusted R2 score : {Adjusted_R2_linear}\")"
      ],
      "metadata": {
        "id": "rukI-b26lQrh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the actual and predicted test data.\n",
        "plt.figure(figsize=(9,5))\n",
        "plt.plot(10**y_pred_linear)\n",
        "plt.plot(np.array(10**y_test))\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.xlabel('Test Data')\n",
        "plt.ylabel(\"Price\")\n",
        "plt.title(\"Actual vs Predicted Closing price Linear regression\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aDKtll7TlSq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to store our performance data for this model so that we can compare them with other models. Let's store them in a dict for now."
      ],
      "metadata": {
        "id": "YmF4-yQplZh9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "linear_regessor_list = {'Mean Absolute Error' : MAE_linear,'Mean squared Error' : MSE_linear,\n",
        "                   'Root Mean squared Error' : RMSE_linear,'R2 score' : R2_linear,'Adjusted R2 score' : Adjusted_R2_linear }"
      ],
      "metadata": {
        "id": "SIWeEunblXp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# converting above dict into a dataframe\n",
        "metric_df = pd.DataFrame.from_dict(linear_regessor_list, orient='index').reset_index()"
      ],
      "metadata": {
        "id": "hnqYGVUulcwu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# renaming the columns.\n",
        "metric_df = metric_df.rename(columns={'index':'Metric',0:'Linear Regression'})\n",
        "metric_df"
      ],
      "metadata": {
        "id": "e2zdpVh_leb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now use this df to store all metrics of all other models so we can easily compare them."
      ],
      "metadata": {
        "id": "uB3-HigRlkQE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Lasso Regression with cross validated regularization."
      ],
      "metadata": {
        "id": "eGTBFQsFlto2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing Lasso model.\n",
        "from sklearn.linear_model import Lasso"
      ],
      "metadata": {
        "id": "GzqHs5Sulggt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing the model with some base values.\n",
        "lasso  = Lasso(alpha=0.0001 , max_iter= 3000)\n",
        "# Fitting the model on our training data.\n",
        "lasso.fit(x_train, y_train)"
      ],
      "metadata": {
        "id": "cqwGaZDnlxdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing the intercept and coefficients.\n",
        "lasso.intercept_"
      ],
      "metadata": {
        "id": "834yWboMl0Im"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lasso.coef_"
      ],
      "metadata": {
        "id": "8lqO6DyQl157"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross validation. optimizing our model by finding the best value of our hyperparameter.\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "lasso_param_grid = {'alpha': [1e-15,1e-13,1e-10,1e-8,1e-5,1e-4,1e-3,0.005,0.006,0.007,0.01,0.015,0.02,1e-1,1,5,10,20,30,40,45,50]}  # list of parameters. \n",
        "                                                                                  \n",
        "lasso_regressor = GridSearchCV(lasso, lasso_param_grid, scoring='neg_mean_squared_error', cv=5)\n",
        "lasso_regressor.fit(x_train, y_train)"
      ],
      "metadata": {
        "id": "z_Vgxw4fl3l0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# getting the best parameter\n",
        "lasso_regressor.best_params_          # after several iterations and trials, we get this value as best parameter value."
      ],
      "metadata": {
        "id": "3PTIDICVl5PB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# getting the best score\n",
        "lasso_regressor.best_score_"
      ],
      "metadata": {
        "id": "mfSx783Rl66M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicting on the test dataset.\n",
        "y_pred_lasso = lasso_regressor.predict(x_test)\n",
        "print(y_pred_lasso)"
      ],
      "metadata": {
        "id": "z62-KZypl9s4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking the performance using evaluation metrics.\n",
        "MAE_lasso = round(mean_absolute_error(10**(y_test),10**(y_pred_lasso)),4)\n",
        "print(f\"Mean Absolute Error : {MAE_lasso}\")\n",
        "\n",
        "MSE_lasso  = round(mean_squared_error(10**(y_test),10**(y_pred_lasso)),4)\n",
        "print(\"Mean squared Error :\" , MSE_lasso)\n",
        "\n",
        "RMSE_lasso = round(np.sqrt(MSE_lasso),4)\n",
        "print(\"Root Mean squared Error :\" ,RMSE_lasso)\n",
        "\n",
        "R2_lasso = round(r2_score(10**(y_test), 10**(y_pred_lasso)),4)\n",
        "print(\"R2 score :\" ,R2_lasso)\n",
        "\n",
        "Adjusted_R2_lasso = round(1-(1-r2_score(10**y_test, 10**y_pred_lasso))*((x_test.shape[0]-1)/(x_test.shape[0]-x_test.shape[1]-1)),4)\n",
        "print(\"Adjusted R2 score: \", Adjusted_R2_lasso)"
      ],
      "metadata": {
        "id": "_j72Tpqel_M6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now saving these metrics to our metrics dataframe. First we save them in a list and then we pass them to the df.\n",
        "metric_df['Lasso'] = [MAE_lasso, MSE_lasso, RMSE_lasso, R2_lasso, Adjusted_R2_lasso]"
      ],
      "metadata": {
        "id": "Ez-k31EmmBNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plotting the predicted values vs actual.\n",
        "plt.figure(figsize=(9,5))\n",
        "plt.plot(10**y_pred_lasso)\n",
        "plt.plot(np.array(10**y_test))\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.ylabel(\"Price\")\n",
        "plt.title(\"Actual vs Predicted Closing price Lasso regression\")"
      ],
      "metadata": {
        "id": "XqXwR5MImDk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Ridge Regression with cross validated regularization."
      ],
      "metadata": {
        "id": "pIrUKFbLmIwz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing ridge regressor model.\n",
        "from sklearn.linear_model import Ridge    \n",
        "ridge = Ridge()         # iitializing the model\n",
        "\n",
        "# initiating the parameter grid for alpha (regularization strength).\n",
        "ridge_param_grid = {'alpha': [1e-15,1e-10,1e-8,1e-5,1e-4,1e-3,1e-2,0.3,0.7,1,1.2,1.33,1.365,1.37,1.375,1.4,1.5,1.6,1.8,2.5,5,10,20,30,40,45,50,55,60,100]}\n",
        "\n",
        "# cross validation. \n",
        "ridge_regressor = GridSearchCV(ridge, ridge_param_grid, scoring='neg_mean_squared_error', cv=3)\n",
        "ridge_regressor.fit(x_train,y_train)"
      ],
      "metadata": {
        "id": "DoPSJpszmFIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# finding the best parameter value (for alpha)\n",
        "ridge_regressor.best_params_"
      ],
      "metadata": {
        "id": "H6kq66l4mN0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# getting the best score for optimal value of alpha.\n",
        "ridge_regressor.best_score_"
      ],
      "metadata": {
        "id": "SBx11Vi3mRKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# predicting on the test dataset now.\n",
        "y_pred_ridge = ridge_regressor.predict(x_test)"
      ],
      "metadata": {
        "id": "1Dr3JdRkmSxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluating performance.\n",
        "MAE_ridge = round(mean_absolute_error(10**(y_test),10**(y_pred_ridge)),4)\n",
        "print(f\"Mean Absolute Error : {MAE_ridge}\")\n",
        "\n",
        "MSE_ridge  = round(mean_squared_error(10**(y_test),10**(y_pred_ridge)),4)\n",
        "print(\"Mean squared Error :\" , MSE_ridge)\n",
        "\n",
        "RMSE_ridge = round(np.sqrt(MSE_ridge),4)\n",
        "print(\"Root Mean squared Error :\" ,RMSE_ridge)\n",
        "\n",
        "R2_ridge = round(r2_score(10**(y_test), 10**(y_pred_ridge)),4)\n",
        "print(\"R2 score :\" ,R2_ridge)\n",
        "\n",
        "Adjusted_R2_ridge = round(1-(1-r2_score(10**y_test, 10**y_pred_ridge))*((x_test.shape[0]-1)/(x_test.shape[0]-x_test.shape[1]-1)),4)\n",
        "print(\"Adjusted R2 score: \", Adjusted_R2_ridge)"
      ],
      "metadata": {
        "id": "uAxlcezCmUTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# storing these values in a list and appending to our metric df.\n",
        "ridge_regressor_list = [MAE_ridge,MSE_ridge,RMSE_ridge,R2_ridge,Adjusted_R2_ridge]\n",
        "metric_df['Ridge'] = ridge_regressor_list"
      ],
      "metadata": {
        "id": "3xUjH6XXmWXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting predicted and actual target variable values.\n",
        "plt.figure(figsize=(9,5))\n",
        "plt.plot(10**y_pred_ridge)\n",
        "plt.plot(np.array(10**y_test))\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.ylabel(\"Price\")\n",
        "plt.title(\"Actual vs Predicted Closing price Ridge regression\")"
      ],
      "metadata": {
        "id": "lHN78JbAmYEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#. Elastic-Net Regression with cross validation."
      ],
      "metadata": {
        "id": "PU-l16bwmdli"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing and initializing Elastic-Net Regression.\n",
        "from sklearn.linear_model import ElasticNet\n",
        "elasticnet_model = ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
        "\n",
        "# initializing parameter grid.\n",
        "elastic_net_param_grid = {'alpha': [1e-15,1e-13,1e-10,1e-8,1e-5,1e-4,1e-3,0.001,0.01,0.02,0.03,0.04,1,5,10,20,40,50,60,100],\n",
        "                          'l1_ratio':[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]}\n",
        "\n",
        "# cross-validation.\n",
        "elasticnet_regressor = GridSearchCV(elasticnet_model, elastic_net_param_grid, scoring='neg_mean_squared_error',cv=5)\n",
        "elasticnet_regressor.fit(x_train, y_train)"
      ],
      "metadata": {
        "id": "7SeC4I4BmZ5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# finding the best parameter\n",
        "elasticnet_regressor.best_params_"
      ],
      "metadata": {
        "id": "fhOF0c-cmmt6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# finding the best score for the optimal parameter.\n",
        "elasticnet_regressor.best_score_"
      ],
      "metadata": {
        "id": "MZBrRSyEmoha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# making the predictions.\n",
        "y_pred_elastic_net = elasticnet_regressor.predict(x_test)"
      ],
      "metadata": {
        "id": "KTG3xWMtmqUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAE_elastic_net = round(mean_absolute_error(10**(y_test),10**(y_pred_elastic_net)),4)\n",
        "print(f\"Mean Absolute Error : {MAE_elastic_net}\")\n",
        "\n",
        "MSE_elastic_net  = round(mean_squared_error(10**(y_test),10**(y_pred_elastic_net)),4)\n",
        "print(\"Mean squared Error :\" , MSE_elastic_net)\n",
        "\n",
        "RMSE_elastic_net = round(np.sqrt(MSE_elastic_net),4)\n",
        "print(\"Root Mean squared Error :\" ,RMSE_elastic_net)\n",
        "\n",
        "R2_elastic_net = round(r2_score(10**(y_test), (10**y_pred_elastic_net)),4)\n",
        "print(\"R2 score :\" ,R2_elastic_net)\n",
        "\n",
        "Adjusted_R2_elastic_net = round(1-(1-r2_score(10**y_test, 10**y_pred_elastic_net))*((x_test.shape[0]-1)/(x_test.shape[0]-x_test.shape[1]-1)),4)\n",
        "print(\"Adjusted R2 score: \", Adjusted_R2_elastic_net)"
      ],
      "metadata": {
        "id": "7Nb0OMGamrvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# storing these metrics in our dataframe.\n",
        "elastic_net_metric_list = [MAE_elastic_net,MSE_elastic_net,RMSE_elastic_net,R2_elastic_net,Adjusted_R2_elastic_net]\n",
        "metric_df['Elastic Net'] = elastic_net_metric_list"
      ],
      "metadata": {
        "id": "kkD1Whsumt67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now let us plot the actual and predicted target variables values.\n",
        "plt.figure(figsize=(9,5))\n",
        "plt.plot(10**y_pred_elastic_net)\n",
        "plt.plot(np.array(10**y_test))\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.ylabel(\"Price\")\n",
        "plt.title(\"Actual vs Predicted Closing price Elastic Net regression\")"
      ],
      "metadata": {
        "id": "MCT3TkGomvye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# comparing the performance of all models that we have implemented.\n",
        "metric_df"
      ],
      "metadata": {
        "id": "ye6gEhDhmx6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From above data, we can clearly see that the best performing model is Elastic \n",
        "\n",
        "Net as it scores the best in every single metric."
      ],
      "metadata": {
        "id": "tmoNwDx0m4kY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the predicted values of all the models against the true values.\n",
        "plt.figure(figsize=(16,8))\n",
        "plt.plot(10**y_test, linewidth=2)\n",
        "plt.plot(10**y_pred_linear)\n",
        "plt.plot(10**y_pred_lasso)\n",
        "plt.plot(10**y_pred_ridge)\n",
        "plt.plot(10**y_pred_elastic_net)\n",
        "plt.legend(['linear','lasso','ridge','elastic_net'])\n",
        "plt.title('Actual vs Predicted Closing Price values by various Algorithms', weight = 'bold',fontsize=16)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ompsSHbSmzeD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see from above graph, all of our models are performing really well and are able to closely approximate the actual values."
      ],
      "metadata": {
        "id": "UAWIDmhym-d-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets check for Heterodasticity. Homoscedasticity is an assumption in linear regression algorithm.\n",
        "# Homoscedasticity means that the model should perform well on all the datapoints.\n",
        "\n",
        "# Plotting the residuals(errors) against actual test data.\n",
        "residuals = 10**y_test - 10**y_pred_elastic_net.reshape(37,1)\n",
        "plt.scatter(10**y_test,residuals,c='red')\n",
        "plt.title('Actual Test data vs Residuals (Elastic Net)')"
      ],
      "metadata": {
        "id": "Eby-OpXXm79N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above graph, we have plotted the residuals against test set value for our actual test set values for our best performing model (Elastic Net Regressor).\n",
        "\n",
        "***As we can see, there is no discernable pattern here in the plot. The errors are similar for all datapoints and the model is performing equally well on all datapoints. So we can say that the assumption of Homoscedasticity is valid in this case.***  "
      ],
      "metadata": {
        "id": "clMUTdZhnENO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# saving the actual and elastic net predicted target variables values in a dataframe. \n",
        "actual_pred_df = pd.DataFrame(10**y_test,10**y_pred_elastic_net).reset_index().rename(columns = {'index':'Actual values',0:'Elastic Net Predicted values'})\n",
        "actual_pred_df.head()"
      ],
      "metadata": {
        "id": "TCSBKubVnAyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Conclusions Drawn :**\n",
        "---\n",
        "\n",
        "*   **Using data visualization on our target variable, we can clearly see the impact of 2018 fraud case involving Rana Kapoor as the stock prices decline dramatically during that period.**\n",
        "*   **After loading the dataset, we found that there are no null values in our dataset nor any duplicate data.**\n",
        "*   **There are some outliers in our features however this being a very small dataset, dropping those instances will lead to loss of information.**\n",
        "*   **We found that the distribution of all our variables is positively skewed. so we performed log transformation on them.**\n",
        "*   **There is a high correlation between the dependent and independent variables. This is a signal that our dependent variable is highly dependent on our features and can be predicted accurately from them.**\n",
        "*   **We found that there is a rather high correlation between our independent variables. This multicollinearity is however unavoidable here as the dataset is very small.**\n",
        "*   **We implemented several models on our dataset in order to be able to predict the closing price and found that all our models are performing remarkably well and *Elastic Net regressor is the best performing model with Adjusted R2 score value of 0.9932* and scores well on all evaluation metrics.** \n",
        "*   **All of the implemented models performed quite well on our data giving us the Adjusted R-square of over 99%.**\n",
        "*   **We checked for presence of Heterodasceticity in our dataset by plotting the residuals against the Elastic Net model predicted value and found that there is no Heterodasceticity present. Our model is performing well on all data-points.**\n",
        "*   **With our model making predictions with such high accuracy, we can confidently deploy this model for further predictive tasks using future data.** "
      ],
      "metadata": {
        "id": "hc_cOL_TnLK7"
      }
    }
  ]
}
